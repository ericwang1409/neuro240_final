@misc{7,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{6,
      title={A Multiscale Visualization of Attention in the Transformer Model}, 
      author={Jesse Vig},
      year={2019},
      eprint={1906.05714},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@article{1,
  author = {Elhage et al.},
  title  = {A Mathematical Framework for Transformer Circuits},
  year   = {2021},
  url    = {https://transformer-circuits.pub/2021/framework/index.html},
  note   = {Accessed: 2024-03-09}
}

@misc{2,
  doi = {10.48550/ARXIV.2301.05217},
  
  url = {https://arxiv.org/abs/2301.05217},
  
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Progress measures for grokking via mechanistic interpretability},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{3,
  author = {Neel Nanda},
  title  = {200 COP in MI: Interpreting Algorithmic Problems},
  year   = {2022},
  url    = {https://www.alignmentforum.org/s/yivyHaCAmMJ3CqSyj/p/ejtFsvyhRkMofKAFy},
  note   = {Accessed: 2024-03-09}
}

