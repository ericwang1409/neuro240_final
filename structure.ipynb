{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter constants\n",
    "N_LAYERS = 1\n",
    "N_HEADS = 1\n",
    "D_MODEL = 32\n",
    "D_HEAD = 32\n",
    "D_MLP = None\n",
    "D_VOCAB = 64\n",
    "SEED = 123\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating lists of a fixed length parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "import random\n",
    "\n",
    "FIXED_LENGTH = 10\n",
    "\n",
    "def generateLists(n, training=True):\n",
    "    output = []\n",
    "    for _ in range(n):\n",
    "        curr = []\n",
    "        for _ in range(FIXED_LENGTH):\n",
    "            if training:\n",
    "                curr.append(random.randint(0, D_VOCAB - 1))\n",
    "            else:\n",
    "                curr.append(random.randint(D_VOCAB // 2, D_VOCAB - 1))\n",
    "            # curr.append(random.randint(0, 100))\n",
    "\n",
    "        # maximum = max(curr)\n",
    "        # output.append((curr, maximum))\n",
    "        output.append(curr)\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data function with separate training and testing data so it doesn't see what\n",
    "it's being tested on. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LENGTH = 10\n",
    "\n",
    "def separate_data(n, split=0.7):\n",
    "    output = []\n",
    "    for i in range(D_VOCAB):\n",
    "        for j in range(D_VOCAB):\n",
    "            curr = [i, j]\n",
    "            output.append(curr)\n",
    "\n",
    "    random.shuffle(output)\n",
    "\n",
    "    split_index = int(len(output) * split)\n",
    "    # return training, testing\n",
    "    return torch.tensor(output[:split_index]), torch.tensor(output[split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(data, batch_size=128):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_generate():\n",
    "    output = []\n",
    "    for i in range(D_VOCAB):\n",
    "        for j in range(D_VOCAB):\n",
    "            curr = [i, j]\n",
    "            output.append(curr)\n",
    "\n",
    "    random.shuffle(output)\n",
    "    return output\n",
    "\n",
    "def cross_val_generate(output, epoch, total_epoch):\n",
    "    testing_size = int(len(output) / total_epoch)\n",
    "    split_index = testing_size * epoch\n",
    "    # return training, testing\n",
    "    testing = torch.tensor(output[split_index:split_index + testing_size])\n",
    "    training = torch.tensor(output[:split_index] + output[split_index + testing_size:])\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model paramters. We are using one layer, one attention head (which pays to the \n",
    "tokens in contex to another), the dimensions of the model, dimension of the head,\n",
    "vocab is the size of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_head=D_HEAD,\n",
    "    n_ctx=FIXED_LENGTH,\n",
    "    d_vocab=D_VOCAB,\n",
    "    act_fn=\"relu\",\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    "    attn_only=True\n",
    ")\n",
    "\n",
    "# hooked transformer used for interpretation later\n",
    "model = HookedTransformer(cfg, move_to_device=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, tokens, return_per_token=True, print_tokens=False):\n",
    "    # we take the last element of the logits to make the next prediction\n",
    "    logits = logits[:, -1, :]\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    log_prob = logits.log_softmax(-1)\n",
    "    if print_tokens:\n",
    "        print(\"tokens\", tokens)\n",
    "        print(\"predicted\", torch.argmax(logits, dim=-1))\n",
    "    # shape is (batch_size, 1) which represents probabilities \n",
    "    # of the correct answer\n",
    "    output_prob = log_prob.gather(-1, answer.unsqueeze(-1))\n",
    "    if return_per_token:\n",
    "        return -1 * output_prob.squeeze()\n",
    "    return -1 * output_prob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, tokens, return_per_token=False):\n",
    "    logits = logits[:, -1, :]\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    if return_per_token:\n",
    "        return (predicted == answer).float()\n",
    "    return (predicted == answer).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1], [2], [3], [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = []\n",
    "        for _ in range(batches_per):\n",
    "            tokens = generateLists(batch_size, training=True)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    # might want to create the training and testing set beforehand\n",
    "    test_data = generateLists(1280, training=True)\n",
    "    logits = model(test_data)\n",
    "    acc = accuracy(logits, test_data, return_per_token=False)\n",
    "\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model2(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    training_data, testing_data = separate_data(batch_size)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        data_generator = output_data(training_data, batch_size)\n",
    "        epoch_losses = []\n",
    "        for _ in range(batches_per):\n",
    "            tokens = next(data_generator)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    # might want to create the training and testing set beforehand\n",
    "    logits = model(testing_data)\n",
    "    acc = accuracy(logits, testing_data, return_per_token=False)\n",
    "\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    all_data = raw_generate()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        tokens, test = cross_val_generate(all_data, epoch, n_epochs - 1)\n",
    "        epoch_losses = []\n",
    "        for _ in range(batches_per):\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        model.eval()\n",
    "        logits = model(test)\n",
    "        acc = accuracy(logits, test, return_per_token=False)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "            print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 4.462099552154541\n",
      "Epoch 10, train loss: 1.206030249595642\n",
      "Epoch 20, train loss: 0.2497977912425995\n",
      "Epoch 30, train loss: 0.09170398861169815\n",
      "Epoch 40, train loss: 0.04693559929728508\n",
      "Epoch 50, train loss: 0.0287907924503088\n",
      "Epoch 60, train loss: 0.01964835822582245\n",
      "Epoch 70, train loss: 0.014348438009619713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, train loss: 0.010973022319376469\n",
      "Epoch 90, train loss: 0.008676661178469658\n",
      "Test accuracy: 0.9568755030632019\n"
     ]
    }
   ],
   "source": [
    "losses = train_model2(model, 100, 128, 10, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
