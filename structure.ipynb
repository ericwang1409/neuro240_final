{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter constants\n",
    "N_LAYERS = 1\n",
    "N_HEADS = 1\n",
    "D_MODEL = 32\n",
    "D_HEAD = 32\n",
    "D_MLP = None\n",
    "D_VOCAB = 64\n",
    "SEED = 123\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating lists of a fixed length parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "import random\n",
    "\n",
    "FIXED_LENGTH = 10\n",
    "\n",
    "def generateLists(n, training=True):\n",
    "    output = []\n",
    "    for _ in range(n):\n",
    "        curr = []\n",
    "        for _ in range(FIXED_LENGTH):\n",
    "            if training:\n",
    "                curr.append(random.randint(0, D_VOCAB - 1))\n",
    "            else:\n",
    "                curr.append(random.randint(D_VOCAB // 2, D_VOCAB - 1))\n",
    "            # curr.append(random.randint(0, 100))\n",
    "\n",
    "        # maximum = max(curr)\n",
    "        # output.append((curr, maximum))\n",
    "        output.append(curr)\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data function with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LENGTH = 10\n",
    "\n",
    "def cross_generation(n, split=0.7):\n",
    "    output = []\n",
    "    for i in range(D_VOCAB):\n",
    "        for j in range(D_VOCAB):\n",
    "            curr = [i, j]\n",
    "            output.append(curr)\n",
    "\n",
    "    random.shuffle(output)\n",
    "\n",
    "    split_index = int(len(output) * split)\n",
    "    # return training, testing\n",
    "    return torch.tensor(output[:split_index]), torch.tensor(output[split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(data, batch_size=128):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model paramters. We are using one layer, one attention head (which pays to the \n",
    "tokens in contex to another), the dimensions of the model, dimension of the head,\n",
    "vocab is the size of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_head=D_HEAD,\n",
    "    n_ctx=FIXED_LENGTH,\n",
    "    d_vocab=D_VOCAB,\n",
    "    act_fn=\"relu\",\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    "    attn_only=True\n",
    ")\n",
    "\n",
    "# hooked transformer used for interpretation later\n",
    "model = HookedTransformer(cfg, move_to_device=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, tokens, return_per_token=True, print_tokens=False):\n",
    "    # we take the last element of the logits to make the next prediction\n",
    "    logits = logits[:, -1, :]\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    log_prob = logits.log_softmax(-1)\n",
    "    if print_tokens:\n",
    "        print(\"tokens\", tokens)\n",
    "        print(\"predicted\", torch.argmax(logits, dim=-1))\n",
    "    # shape is (batch_size, 1) which represents probabilities \n",
    "    # of the correct answer\n",
    "    output_prob = log_prob.gather(-1, answer.unsqueeze(-1))\n",
    "    if return_per_token:\n",
    "        return -1 * output_prob.squeeze()\n",
    "    return -1 * output_prob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, tokens, return_per_token=False):\n",
    "    logits = logits[:, -1, :]\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    if return_per_token:\n",
    "        return (predicted == answer).float()\n",
    "    return (predicted == answer).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1], [2], [3], [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = []\n",
    "        for _ in range(batches_per):\n",
    "            tokens = generateLists(batch_size, training=True)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    # might want to create the training and testing set beforehand\n",
    "    test_data = generateLists(1280, training=True)\n",
    "    logits = model(test_data)\n",
    "    acc = accuracy(logits, test_data, return_per_token=False)\n",
    "\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model2(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    training_data, testing_data = cross_generation(batch_size)\n",
    "    print(training_data.shape)\n",
    "    print(testing_data.shape)\n",
    "    for epoch in range(n_epochs):\n",
    "        data_generator = output_data(training_data, batch_size)\n",
    "        epoch_losses = []\n",
    "        for _ in range(batches_per):\n",
    "            tokens = next(data_generator)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    # might want to create the training and testing set beforehand\n",
    "    logits = model(testing_data)\n",
    "    acc = accuracy(logits, testing_data, return_per_token=False)\n",
    "\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2867, 2])\n",
      "torch.Size([1229, 2])\n",
      "Epoch 0, train loss: 4.462099552154541\n",
      "Epoch 10, train loss: 1.206030249595642\n",
      "Epoch 20, train loss: 0.2497977912425995\n",
      "Epoch 30, train loss: 0.09170398861169815\n",
      "Epoch 40, train loss: 0.04693559929728508\n",
      "Epoch 50, train loss: 0.0287907924503088\n",
      "Epoch 60, train loss: 0.01964835822582245\n",
      "Epoch 70, train loss: 0.014348438009619713\n",
      "Epoch 80, train loss: 0.010973022319376469\n",
      "Epoch 90, train loss: 0.008676661178469658\n",
      "Test accuracy: 0.9568755030632019\n"
     ]
    }
   ],
   "source": [
    "losses = train_model2(model, 100, 128, 10, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
