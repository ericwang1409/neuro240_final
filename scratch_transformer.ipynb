{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=64, dim_model=32):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dim_model)\n",
    "        self.query = nn.Linear(dim_model, dim_model)\n",
    "        self.key = nn.Linear(dim_model, dim_model)\n",
    "        self.value = nn.Linear(dim_model, dim_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_model * 4, dim_model)\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attn_weights = F.softmax(q @ k.transpose(-2, -1) / (32 ** 0.5), dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "\n",
    "        ffn_output = self.ffn(attn_output + x)\n",
    "\n",
    "        logits = self.out(ffn_output)\n",
    "        return logits\n",
    "\n",
    "model = SimpleTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "FIXED_LENGTH = 3\n",
    "\n",
    "def generateLists(n):\n",
    "    output = []\n",
    "    for _ in range(n):\n",
    "        curr = []\n",
    "        for _ in range(FIXED_LENGTH):\n",
    "            curr.append(random.randint(0, 64 - 1))\n",
    "            # curr.append(random.randint(0, 100))\n",
    "\n",
    "        # maximum = max(curr)\n",
    "        # output.append((curr, maximum))\n",
    "        output.append(curr)\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(n, split=0.7):\n",
    "    output = []\n",
    "    for i in range(64):\n",
    "        for j in range(64):\n",
    "            curr = [i, j]\n",
    "            output.append(curr)\n",
    "\n",
    "    random.shuffle(output)\n",
    "\n",
    "    split_index = int(len(output) * split)\n",
    "    # return training, testing\n",
    "    return torch.tensor(output[:split_index]), torch.tensor(output[split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(data, batch_size=128):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, tokens, return_per_token=True, print_tokens=False):\n",
    "    # we take the last element of the logits to make the next prediction\n",
    "    logits = logits[:, -1, :]\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    log_prob = logits.log_softmax(-1)\n",
    "    if print_tokens:\n",
    "        print(\"tokens\", tokens)\n",
    "        print(\"predicted\", torch.argmax(logits, dim=-1))\n",
    "    # shape is (batch_size, 1) which represents probabilities \n",
    "    # of the correct answer\n",
    "    output_prob = log_prob.gather(-1, answer.unsqueeze(-1))\n",
    "    if return_per_token:\n",
    "        return -1 * output_prob.squeeze()\n",
    "    return -1 * output_prob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, tokens, return_per_token=False):\n",
    "    logits = logits[:, -1, :]\n",
    "    predicted = torch.argmax(logits, dim=-1)\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    print(predicted, answer)\n",
    "    if return_per_token:\n",
    "        return (predicted == answer).float()\n",
    "    return (predicted == answer).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    training, testing = separate_data(64)\n",
    "    print(training.shape, testing.shape)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        generator = output_data(training, batch_size)\n",
    "        for _ in range(batches_per):\n",
    "            tokens = next(generator)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    logits = model(testing)\n",
    "    acc = accuracy(logits, testing, return_per_token=False)\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2867, 2]) torch.Size([1229, 2])\n",
      "Epoch 0, train loss: 4.115307807922363\n",
      "Epoch 10, train loss: 1.2805118560791016\n",
      "Epoch 20, train loss: 0.2150605022907257\n",
      "Epoch 30, train loss: 0.05243371054530144\n",
      "Epoch 40, train loss: 0.019432011991739273\n",
      "Epoch 50, train loss: 0.009694280102849007\n",
      "Epoch 60, train loss: 0.005810381378978491\n",
      "Epoch 70, train loss: 0.003874266054481268\n",
      "Epoch 80, train loss: 0.00276588904671371\n",
      "Epoch 90, train loss: 0.002070572692900896\n",
      "Epoch 100, train loss: 0.0016054243315011263\n",
      "Epoch 110, train loss: 0.0012786707375198603\n",
      "Epoch 120, train loss: 0.001040170551277697\n",
      "Epoch 130, train loss: 0.0008607251802459359\n",
      "Epoch 140, train loss: 0.0007224645232781768\n",
      "Epoch 150, train loss: 0.000613644253462553\n",
      "Epoch 160, train loss: 0.0005265054060146213\n",
      "Epoch 170, train loss: 0.00045569968642666936\n",
      "Epoch 180, train loss: 0.0003973893471993506\n",
      "Epoch 190, train loss: 0.00034885111381299794\n",
      "Epoch 200, train loss: 0.0003080289752688259\n",
      "Epoch 210, train loss: 0.00027342638350091875\n",
      "Epoch 220, train loss: 0.00024382825358770788\n",
      "Epoch 230, train loss: 0.0002183317265007645\n",
      "Epoch 240, train loss: 0.00019623021944426\n",
      "Epoch 250, train loss: 0.00017694252892397344\n",
      "Epoch 260, train loss: 0.00016003874770831317\n",
      "Epoch 270, train loss: 0.0001451626158086583\n",
      "Epoch 280, train loss: 0.00013201063848100603\n",
      "Epoch 290, train loss: 0.00012033747043460608\n",
      "Epoch 300, train loss: 0.0001099349683499895\n",
      "Epoch 310, train loss: 0.00010062665387522429\n",
      "Epoch 320, train loss: 9.227765258401632e-05\n",
      "Epoch 330, train loss: 8.476796210743487e-05\n",
      "Epoch 340, train loss: 7.799147715559229e-05\n",
      "Epoch 350, train loss: 7.186800939962268e-05\n",
      "Epoch 360, train loss: 6.631096039200202e-05\n",
      "Epoch 370, train loss: 6.126208609202877e-05\n",
      "Epoch 380, train loss: 5.666277138516307e-05\n",
      "Epoch 390, train loss: 5.247721492196433e-05\n",
      "Epoch 400, train loss: 4.865097071160562e-05\n",
      "Epoch 410, train loss: 4.514661122811958e-05\n",
      "Epoch 420, train loss: 4.193028144072741e-05\n",
      "Epoch 430, train loss: 3.897666101693176e-05\n",
      "Epoch 440, train loss: 3.625587487476878e-05\n",
      "Epoch 450, train loss: 3.375610685907304e-05\n",
      "Epoch 460, train loss: 3.14505523419939e-05\n",
      "Epoch 470, train loss: 2.9322540285647847e-05\n",
      "Epoch 480, train loss: 2.7358488296158612e-05\n",
      "Epoch 490, train loss: 2.5537632609484717e-05\n",
      "Epoch 500, train loss: 2.3857563064666465e-05\n",
      "Epoch 510, train loss: 2.2295647795544937e-05\n",
      "Epoch 520, train loss: 2.084490733977873e-05\n",
      "Epoch 530, train loss: 1.9503015209920704e-05\n",
      "Epoch 540, train loss: 1.8253678717883304e-05\n",
      "Epoch 550, train loss: 1.708916897769086e-05\n",
      "Epoch 560, train loss: 1.600818541191984e-05\n",
      "Epoch 570, train loss: 1.5003650332801044e-05\n",
      "Epoch 580, train loss: 1.4065508366911672e-05\n",
      "Epoch 590, train loss: 1.318910472036805e-05\n",
      "Epoch 600, train loss: 1.2371274351608008e-05\n",
      "Epoch 610, train loss: 1.1611550689849537e-05\n",
      "Epoch 620, train loss: 1.0901367204496637e-05\n",
      "Epoch 630, train loss: 1.0237185961159412e-05\n",
      "Epoch 640, train loss: 9.613325346435886e-06\n",
      "Epoch 650, train loss: 9.030532964970917e-06\n",
      "Epoch 660, train loss: 8.486853403155692e-06\n",
      "Epoch 670, train loss: 7.976605047588237e-06\n",
      "Epoch 680, train loss: 7.499042112613097e-06\n",
      "Epoch 690, train loss: 7.054074558254797e-06\n",
      "Epoch 700, train loss: 6.634994406340411e-06\n",
      "Epoch 710, train loss: 6.240965831239009e-06\n",
      "Epoch 720, train loss: 5.871801022294676e-06\n",
      "Epoch 730, train loss: 5.525639608094934e-06\n",
      "Epoch 740, train loss: 5.201641670282697e-06\n",
      "Epoch 750, train loss: 4.8953379518934526e-06\n",
      "Epoch 760, train loss: 4.611944405041868e-06\n",
      "Epoch 770, train loss: 4.342146439739736e-06\n",
      "Epoch 780, train loss: 4.089298272447195e-06\n",
      "Epoch 790, train loss: 3.852654117508791e-06\n",
      "Epoch 800, train loss: 3.629513912528637e-06\n",
      "Epoch 810, train loss: 3.420249868213432e-06\n",
      "Epoch 820, train loss: 3.2216953513852786e-06\n",
      "Epoch 830, train loss: 3.036644557141699e-06\n",
      "Epoch 840, train loss: 2.8632352950808126e-06\n",
      "Epoch 850, train loss: 2.699604465306038e-06\n",
      "Epoch 860, train loss: 2.544634753576247e-06\n",
      "Epoch 870, train loss: 2.400840457994491e-06\n",
      "Epoch 880, train loss: 2.261423105665017e-06\n",
      "Epoch 890, train loss: 2.1334610664780485e-06\n",
      "Epoch 900, train loss: 2.0127631614741404e-06\n",
      "Epoch 910, train loss: 1.8995153823198052e-06\n",
      "Epoch 920, train loss: 1.7925074189406587e-06\n",
      "Epoch 930, train loss: 1.6916462755034445e-06\n",
      "Epoch 940, train loss: 1.594416971784085e-06\n",
      "Epoch 950, train loss: 1.50631478845753e-06\n",
      "Epoch 960, train loss: 1.419795580659411e-06\n",
      "Epoch 970, train loss: 1.3397024076766684e-06\n",
      "Epoch 980, train loss: 1.2643589570870972e-06\n",
      "Epoch 990, train loss: 1.1949759937124327e-06\n",
      "tensor([28, 56, 49, 20, 18, 63, 60, 28, 16, 48, 47, 34, 39, 32, 49, 17, 23, 63,\n",
      "        48, 59, 36, 41, 39, 55, 51, 28, 39, 42, 44, 45, 42, 21, 38, 59, 12, 49,\n",
      "        52, 37, 25, 45, 36, 18, 25, 60, 51, 47, 20, 31, 19, 61, 48, 49, 44, 51,\n",
      "        59, 28, 25, 15, 46, 47, 51, 37, 46, 37, 38, 21, 44, 56, 36, 63, 33, 29,\n",
      "        28, 33, 54, 49, 46, 45, 35, 31, 30, 33, 35, 49, 57, 26, 26, 54, 38, 13,\n",
      "        37, 12, 42, 61, 24, 39, 59, 56, 38, 36, 51, 52, 36, 30, 17, 63, 35, 55,\n",
      "        27, 54, 59, 41, 29, 55, 40, 19, 33, 55, 42, 25, 51, 55, 61, 46, 58, 46,\n",
      "        30, 26, 31, 58, 50, 27, 36, 60, 49, 36, 59, 45, 53, 34, 38, 41, 63, 49,\n",
      "        31, 46, 58, 44, 23, 25, 60, 48, 53, 24, 38, 51,  4, 56, 27, 43, 56, 48,\n",
      "        49, 50, 26, 45, 49, 50, 57, 61, 61, 39, 34, 32,  8, 50, 55, 32, 59, 51,\n",
      "        13, 54, 49, 19, 59, 15, 26, 60, 20, 30, 50,  7, 44, 11, 45, 33, 56, 61,\n",
      "        38, 46, 52, 33, 35, 25, 40, 21, 60, 44, 22, 25, 54, 53, 48, 32, 48, 61,\n",
      "        37, 25, 41, 56, 32, 16, 26, 46, 41, 40, 53, 17, 49, 54, 50, 15, 56, 51,\n",
      "        50, 34, 43, 37, 27, 27, 22, 59, 37, 39, 62, 20,  4, 55, 15, 59,  9, 61,\n",
      "        32, 61, 60, 51, 49, 35, 15, 46, 63, 43, 17, 34, 49, 55, 22,  8, 14, 49,\n",
      "        44, 20, 54, 18, 58, 62, 35, 50, 44, 37, 34, 15, 43, 57, 30, 62, 22, 61,\n",
      "        46, 28, 63, 62, 55, 23, 12, 48, 36, 61, 30, 61, 41, 62, 53, 46, 42, 60,\n",
      "        43, 57, 33, 57, 48, 29, 33, 59, 58,  7, 54, 60, 26,  9, 53, 49, 36, 27,\n",
      "        56, 31, 17, 59, 46, 43, 36, 39, 49, 46, 40, 15, 51, 56, 46, 57, 56, 17,\n",
      "        20, 48, 55, 60, 43, 38, 59, 58, 16, 50, 36, 58, 38, 37, 32, 61, 19, 50,\n",
      "        15, 49, 24, 62, 52, 20, 43, 46, 35, 52, 20, 31, 54, 31, 27, 39, 59, 57,\n",
      "        49, 60, 60, 37, 51, 49, 55, 22, 23, 20, 33, 50, 62, 47, 57, 49, 17, 22,\n",
      "        33, 43, 35, 12, 13, 46, 23, 59, 61, 53, 62, 62, 43, 61, 33, 35, 35, 62,\n",
      "        57, 33, 37, 62, 45, 48, 15, 62, 36, 32, 52, 41, 45, 51, 40, 40, 53, 50,\n",
      "        55, 28, 37, 54, 34, 24, 53, 21, 37, 62, 61, 40, 59, 48, 53, 37, 29, 60,\n",
      "        36, 58, 32, 53,  8, 40, 29, 62, 56, 38, 45, 45, 43, 56, 52, 58, 42, 55,\n",
      "        38, 50, 32, 50, 37, 39, 52, 63, 40, 51, 54, 12,  9, 60, 25, 36, 27, 47,\n",
      "        31, 53, 19, 32, 47, 43, 53, 26, 37, 30, 43, 63, 62, 34, 44, 42, 18, 29,\n",
      "        58, 54, 61, 60, 32, 46, 57, 25, 43, 17, 56, 18, 59, 33, 49, 60, 57, 36,\n",
      "        61, 35, 55, 42, 47, 32, 39, 42, 49,  4, 48, 25, 24, 59, 20, 41, 55, 17,\n",
      "        34, 13, 58, 40, 42, 59, 32, 57, 25, 51, 29, 46, 16, 49, 60, 51, 53,  7,\n",
      "        50, 46, 56, 25, 29, 54, 16, 56, 59, 41, 56, 22, 47, 34, 34, 55, 44, 54,\n",
      "        56, 44, 38, 56, 38, 62, 30, 56, 37, 50, 43, 43, 43, 37, 19, 38, 30, 11,\n",
      "        57, 60, 61, 37, 43, 29, 54, 20, 44, 43, 39, 44, 27, 25, 53, 55, 29, 58,\n",
      "        60, 37, 29, 58, 36, 52, 26, 61, 44,  3, 38, 49, 55, 35, 52, 42, 53, 34,\n",
      "        30, 38, 41, 43, 44, 40, 59, 35,  4, 50, 58, 35, 21, 15, 30, 22, 57, 36,\n",
      "        31, 39, 47, 47, 38, 57, 28, 33, 55, 41, 36, 45, 50, 11, 43, 44,  3, 15,\n",
      "        33, 11, 42, 54, 54, 51, 57, 57, 19, 34, 33, 51, 55, 38, 53, 22, 52, 53,\n",
      "        58, 55, 49, 54, 28, 52, 53, 49, 47, 10, 51, 57, 56, 54, 17, 32, 47, 46,\n",
      "        47, 39, 56, 47, 50, 63, 38, 26, 11, 41, 44, 58, 62, 17, 39, 42, 54, 26,\n",
      "        48, 34,  1, 63, 30, 27, 42, 34, 47, 60, 25, 59, 32, 43, 14, 45, 34, 57,\n",
      "        43, 49, 58,  8, 58, 38, 43, 42, 31, 61, 12, 15, 57, 41, 34, 35, 20, 56,\n",
      "        34, 42, 58, 35, 14, 34, 24, 55, 43, 35, 29, 63, 12, 50, 53, 33, 46, 30,\n",
      "        46, 55, 55, 40, 30, 42, 44, 59, 61, 55, 62, 57,  3, 58, 51,  8, 56, 25,\n",
      "        52, 47, 56, 17, 50, 40, 44, 56, 43, 36, 27, 62, 54, 21, 52, 13, 42, 58,\n",
      "        59, 33, 43, 53, 57, 41, 45, 33, 57, 51, 51, 17, 59, 52, 25, 62, 20, 57,\n",
      "        56, 16, 43, 61, 43, 45, 58,  4, 14, 44, 41, 30, 43, 29, 21, 57, 48, 17,\n",
      "        27, 37, 53, 51, 43, 46, 46, 32, 36, 52, 22, 55, 49, 63, 58, 60, 62, 58,\n",
      "        57, 27, 22, 27, 15, 48, 63, 58, 50, 40, 50, 53, 40, 43, 44, 53, 30, 58,\n",
      "        41, 62, 46, 46, 29, 46, 44, 61, 41, 55, 40, 36, 59, 12, 13, 28, 41, 34,\n",
      "        33, 17, 55, 61, 61, 22, 55, 32, 54, 54, 57,  9, 20, 51, 60, 33, 56, 61,\n",
      "        62, 42, 42, 61, 39, 38, 29, 44, 58, 11, 35, 63, 34, 60, 55, 27, 24, 47,\n",
      "        58, 53, 57, 38, 12, 36, 43, 54, 20, 46, 46, 60, 57, 23, 62, 53, 54, 55,\n",
      "        46, 34, 58, 36, 60, 50, 44, 44, 32, 54, 27, 36, 52, 40, 59, 25, 53, 50,\n",
      "        14, 61, 55, 39, 58, 56,  1, 46, 50, 51, 56, 38, 46, 30, 62, 48, 40, 63,\n",
      "        15, 38, 37, 41, 53, 48, 51, 39, 49, 31, 18, 15, 55, 41, 44, 27, 59, 31,\n",
      "        21, 60, 41, 27, 42, 57, 45, 38, 35, 10, 35, 34, 33,  7, 62, 38, 11,  7,\n",
      "        22, 58, 40, 28, 51, 41, 55, 20, 57, 54, 15, 34, 49, 37, 58, 34, 40, 62,\n",
      "        53, 46,  8, 16, 30, 40, 31, 32, 52, 23, 23, 61, 25, 61, 36, 34, 50, 55,\n",
      "        56, 48, 38, 54, 20, 53, 40, 32, 52, 49, 55, 12, 55, 60, 23, 41, 26, 33,\n",
      "        43, 48, 13, 26, 39, 36, 19, 31, 60, 44, 55, 48, 45, 57,  8, 28, 25, 63,\n",
      "        40, 47, 47, 61, 57, 15, 15, 50, 11, 57, 54, 24, 42, 47, 18, 61, 41, 17,\n",
      "        30, 27, 58, 31, 10, 60, 46, 33, 52, 41, 17, 50, 20, 37, 14, 31, 24, 33,\n",
      "        45, 43, 32, 60, 35, 59, 23, 19,  8, 60, 20, 46, 52, 62, 61, 33, 28, 32,\n",
      "        45, 36, 39, 47, 61, 32, 48, 54, 38, 55, 53, 19, 42, 34, 46, 12, 54, 52,\n",
      "        47, 16, 30, 57, 23, 48, 38, 60, 42, 46, 40, 42, 42,  8, 47, 40, 39, 53,\n",
      "        57, 51, 35, 38, 54, 49, 51, 37, 15,  1, 61, 63, 51, 16, 55, 51, 21, 42,\n",
      "        36, 52,  8, 55, 50, 57, 60, 50, 30, 62, 25, 28, 52, 42, 29, 47, 36, 19,\n",
      "        38, 46, 53, 51, 30]) tensor([31, 56, 49, 20, 18, 63, 60, 28, 16, 48, 47, 34, 39, 32, 49, 24, 23, 63,\n",
      "        48, 59, 36, 41, 39, 55, 51, 28, 39, 42, 47, 45, 42, 21, 38, 59, 16, 49,\n",
      "        52, 37, 25, 57, 36, 18, 29, 60, 51, 47, 20, 31, 19, 61, 48, 49, 21, 51,\n",
      "        24, 28, 25, 37, 46, 47, 51, 38, 46, 37, 38, 21, 44, 56, 36, 63, 33, 29,\n",
      "        28, 33, 54, 49, 46, 45, 35, 31, 30, 33, 35, 49, 57, 26, 38, 54, 38, 16,\n",
      "        63, 29, 42, 61, 24, 39, 59, 56, 38, 36, 51, 52, 36, 30, 33, 63, 35, 59,\n",
      "        27, 54, 59, 41, 29, 55, 40, 19, 33, 55, 45, 25, 51, 55, 61, 46, 58, 57,\n",
      "        30, 29, 31, 58, 50, 27, 36, 60, 49, 36, 59, 45, 53, 34, 38, 41, 63, 49,\n",
      "        31, 16, 58, 16, 23, 25, 60, 48, 53, 24, 55, 51,  3, 56, 27, 43, 56, 48,\n",
      "        49, 50, 26, 45, 49, 61, 57, 61, 42, 39, 34, 32,  8, 57, 55, 32, 59, 51,\n",
      "        13, 54, 49, 19, 59, 15, 26, 60, 20, 30, 50,  7, 44, 11, 63, 33, 56, 61,\n",
      "        38, 46, 52, 33, 35, 25, 40, 21, 60, 44, 22, 25, 54, 53, 48, 32, 48, 61,\n",
      "        37, 25, 41, 56, 63, 16, 26, 46, 47, 40, 53, 17, 49, 54, 50, 15, 56, 51,\n",
      "        50, 34, 43, 37, 27, 27, 22, 59, 37, 39, 62, 20,  4, 55, 15, 63,  9, 61,\n",
      "        57, 61, 60, 51, 49, 35, 16, 46, 63, 43, 17, 53, 49, 55, 22,  8, 14, 49,\n",
      "        44, 20, 55, 29, 58, 62, 35, 54, 44, 37, 34, 15, 43, 57, 30, 62, 22, 61,\n",
      "        46, 29, 63, 62, 55, 23, 36, 48, 36, 61, 26, 61, 41, 62, 53, 21, 60, 60,\n",
      "        47, 57, 33, 57, 48, 36, 33, 59, 58, 21, 54, 60, 26, 11, 53, 49, 36, 25,\n",
      "        56, 31, 22, 59, 46, 45, 36, 63, 49, 46, 14, 15,  6, 56, 33, 57, 56, 17,\n",
      "        23, 48, 24, 60, 43, 38, 59, 58, 16, 58, 36, 58, 38, 37, 60, 61, 19, 51,\n",
      "        15, 55, 32, 62, 52, 29, 43, 46, 54, 52, 20, 31, 54, 31, 30, 39, 59, 57,\n",
      "        49, 60, 60, 37, 51, 49, 55, 22, 23, 27, 33, 50, 62, 47, 57, 49, 17, 22,\n",
      "        54, 43, 35, 12, 13, 33, 24, 59, 61, 53, 62, 62, 57, 61, 33, 10, 35, 62,\n",
      "        57, 33, 42, 62, 46, 48, 15, 62, 36, 32, 52, 41, 45, 51, 40, 60, 53, 50,\n",
      "        55, 28, 37, 54, 34, 24, 53,  9, 37, 62, 61, 40, 59, 48, 53, 37, 29, 62,\n",
      "        36, 58, 32, 53, 14, 40, 29, 62, 56, 38, 45, 45, 43, 56, 52, 58, 42, 55,\n",
      "        38, 50, 32, 50, 37, 39, 63, 63, 40, 51, 54, 42, 24, 60, 25, 36, 27, 47,\n",
      "        31, 53, 63, 32, 47, 60, 53, 26, 39, 35, 47, 63, 62, 34, 44, 42, 18, 29,\n",
      "        61, 54, 61, 60, 32, 48, 57, 25, 43, 17, 56, 18, 59, 33, 49, 60, 57, 36,\n",
      "        61, 35, 55, 42, 47, 32, 39, 42, 50, 12, 48, 53, 24, 59, 20, 41, 55, 17,\n",
      "        26, 13, 58, 40, 42, 59, 32, 57, 41, 51, 29, 46, 16, 49, 60, 51, 53, 21,\n",
      "        50, 46, 56, 25, 29, 54, 16, 56, 59, 41, 56, 22, 47, 34, 34, 55, 44, 54,\n",
      "        56, 44, 38, 56,  5, 62, 30, 56, 37, 50, 43, 43, 43, 37, 19, 46, 30, 11,\n",
      "        57, 60, 61, 37, 43, 29,  5, 20, 44, 43, 39, 44, 27, 25, 53, 55, 29, 58,\n",
      "        60, 37, 29, 58, 39, 52, 26, 61, 44,  6, 38, 49, 55, 35, 52, 42, 53, 34,\n",
      "        30, 38, 41, 43, 44, 40, 59, 35,  2, 55, 58, 35, 21, 15, 30, 22, 57, 36,\n",
      "        31, 39, 47, 47, 38, 57, 28, 34, 55, 53, 36, 45, 50, 11, 43, 44,  5, 15,\n",
      "        33, 11, 42, 54, 54, 51, 57, 57, 19, 34, 36, 51, 55, 63, 53, 22, 52, 53,\n",
      "        58, 55, 49, 54, 28, 52, 53, 49, 47, 10, 51, 57, 56, 54, 19, 32, 47, 46,\n",
      "        47, 39, 56, 47, 50, 63, 38, 26, 11, 47, 44, 58, 62, 24, 39, 42, 54, 26,\n",
      "        48, 34, 49, 63, 30, 27, 42, 34, 47, 61, 53, 59, 32, 43, 14, 45, 34, 57,\n",
      "        43, 49, 58,  8, 58, 38, 54, 47, 21, 61, 12, 15, 57, 41, 34, 35, 21, 56,\n",
      "        34, 42, 58, 35, 35, 34, 24, 55, 59, 35, 29, 63, 12, 50, 53, 33, 46, 30,\n",
      "        46, 55, 55, 40, 30, 42, 44, 59, 61, 55, 62, 57, 10, 58, 51,  5, 56, 25,\n",
      "        52, 47, 56, 17, 50, 40, 44, 58, 43, 36, 27, 62, 54, 21, 52, 21, 47, 58,\n",
      "        59, 33, 43, 53, 57, 41, 45, 33, 57, 47, 58, 21, 59, 52, 26, 62, 20, 57,\n",
      "        56, 16, 43, 61, 43, 45, 58,  2, 14, 44, 41, 30, 43, 26, 21, 57, 48, 23,\n",
      "        28, 37, 53, 43, 46,  5, 46, 32, 36, 52, 36, 55, 49, 63, 58, 60, 62, 58,\n",
      "        57, 27, 22, 27, 15, 48, 63, 58, 50, 40, 50, 53, 40, 43, 44, 53, 30, 58,\n",
      "        45, 62, 46, 46, 29, 46, 44, 61, 44, 55, 40, 36, 59, 31, 22, 28, 41, 34,\n",
      "        55, 57, 55, 61, 63, 22, 55, 32, 54, 54, 57,  9, 35, 51, 60, 33, 56, 61,\n",
      "        62, 42, 42, 61, 39, 41, 29, 44, 58, 11, 12, 63, 59, 60, 55, 24, 24, 47,\n",
      "        58, 58, 57, 38, 12, 36, 43, 54, 20, 46, 46, 60, 57, 25, 62, 53, 54, 55,\n",
      "        47, 28, 58, 36, 60, 50, 44, 44, 32, 59, 27, 36, 52, 40, 59, 25, 53, 50,\n",
      "        14, 61, 55, 16, 58, 56,  3, 46, 50, 51, 56, 38, 46, 30, 62, 48, 40, 63,\n",
      "        24, 38, 37, 41, 53, 48, 51, 39, 54, 31, 26, 15, 56, 41, 44, 38, 59, 31,\n",
      "        24, 60, 41, 27, 42, 57, 45, 38, 54, 10, 49, 30, 33,  9, 62, 38, 11,  7,\n",
      "        22, 58, 40, 28, 36, 43, 55, 26, 57, 54, 22, 34, 49, 37, 58, 34, 40, 62,\n",
      "        53, 46, 30, 16, 30, 40, 31, 32, 52,  9, 19, 61, 25, 61, 63, 34, 50, 55,\n",
      "        56, 48, 44, 54, 11, 53, 40, 32, 52, 49, 55, 12, 55, 60, 23, 41, 30,  6,\n",
      "        53, 46, 16, 26, 39, 36, 19, 31, 60, 44, 55, 48, 45, 57, 13, 28, 27, 63,\n",
      "        40, 47, 47, 61, 57, 15, 15, 50, 11, 57, 54, 24, 42, 47, 18, 61, 16, 28,\n",
      "        30, 27, 58, 31, 10, 60, 46, 33, 52, 41, 17, 50, 20, 37, 14, 31, 24, 33,\n",
      "        45, 43, 32, 60, 52, 59, 36, 19, 14, 60, 37, 46, 52, 62, 61, 33, 28, 35,\n",
      "        45, 36, 39, 47, 61, 41, 48, 54, 38, 55, 53, 19, 42, 34, 46, 33, 54, 52,\n",
      "        10, 16, 63, 57, 23, 48, 38, 60, 42, 55, 40, 42, 42,  5, 47, 40, 39, 53,\n",
      "        57, 51, 52, 38, 54, 49, 51, 47, 19,  6, 61, 63, 51, 48, 55, 51, 21, 42,\n",
      "        36, 52,  8, 55, 50, 57, 60, 50, 32, 62, 25, 28, 52, 42, 31, 47, 36, 29,\n",
      "        38, 46, 53, 51, 30])\n",
      "Test accuracy: 0.8388934135437012\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=10000)\n",
    "losses = train_model(model, 1000, 128, 10, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
