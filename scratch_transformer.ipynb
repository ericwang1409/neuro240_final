{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# transformer model is defined\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=64, dim_model=32):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dim_model)\n",
    "        self.query = nn.Linear(dim_model, dim_model)\n",
    "        self.key = nn.Linear(dim_model, dim_model)\n",
    "        self.value = nn.Linear(dim_model, dim_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_model * 4, dim_model)\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attn_weights = F.softmax(q @ k.transpose(-2, -1) / (32 ** 0.5), dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "\n",
    "        ffn_output = self.ffn(attn_output + x)\n",
    "\n",
    "        logits = self.out(ffn_output)\n",
    "        return logits\n",
    "\n",
    "model = SimpleTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "FIXED_LENGTH = 3\n",
    "\n",
    "def generateLists(n):\n",
    "    output = []\n",
    "    for _ in range(n):\n",
    "        curr = []\n",
    "        for _ in range(FIXED_LENGTH):\n",
    "            curr.append(random.randint(0, 64 - 1))\n",
    "\n",
    "        output.append(curr)\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(n, split=0.7):\n",
    "    output = []\n",
    "    for i in range(64):\n",
    "        for j in range(64):\n",
    "            curr = [i, j]\n",
    "            output.append(curr)\n",
    "\n",
    "    random.shuffle(output)\n",
    "\n",
    "    split_index = int(len(output) * split)\n",
    "    return torch.tensor(output[:split_index]), torch.tensor(output[split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(data, batch_size=128):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, tokens, return_per_token=True, print_tokens=False):\n",
    "    # we take the last element of the logits to make the next prediction\n",
    "    logits = logits[:, -1, :]\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    log_prob = logits.log_softmax(-1)\n",
    "    if print_tokens:\n",
    "        print(\"tokens\", tokens)\n",
    "        print(\"predicted\", torch.argmax(logits, dim=-1))\n",
    "    # shape is (batch_size, 1) which represents probabilities \n",
    "    # of the correct answer\n",
    "    output_prob = log_prob.gather(-1, answer.unsqueeze(-1))\n",
    "    if return_per_token:\n",
    "        return -1 * output_prob.squeeze()\n",
    "    return -1 * output_prob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, tokens, return_per_token=False):\n",
    "    logits = logits[:, -1, :]\n",
    "    predicted = torch.argmax(logits, dim=-1)\n",
    "    answer = torch.max(tokens, dim=1)[0]\n",
    "    print(predicted, answer)\n",
    "    if return_per_token:\n",
    "        return (predicted == answer).float()\n",
    "    return (predicted == answer).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs, batch_size, batches_per, sequence_length=2):\n",
    "    lr = 1e-3\n",
    "    betas = (0.9, 0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    train_losses = []\n",
    "    training, testing = separate_data(64)\n",
    "    print(training.shape, testing.shape)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        generator = output_data(training, batch_size)\n",
    "        for _ in range(batches_per):\n",
    "            tokens = next(generator)\n",
    "            logits = model(tokens)\n",
    "            # print(tokens.shape)\n",
    "            # print(logits.shape)\n",
    "            losses = loss_function(logits, tokens, print_tokens=False)\n",
    "            losses.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_losses.extend(losses.detach())\n",
    "\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {train_losses[-1]}\")\n",
    "\n",
    "    model.eval()\n",
    "    logits = model(testing)\n",
    "    acc = accuracy(logits, testing, return_per_token=False)\n",
    "    print(f\"Test accuracy: {acc}\")\n",
    "\n",
    "    return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2867, 2]) torch.Size([1229, 2])\n",
      "Epoch 0, train loss: 4.106841087341309\n",
      "Epoch 10, train loss: 1.460169792175293\n",
      "Epoch 20, train loss: 0.2636513113975525\n",
      "Epoch 30, train loss: 0.06560500711202621\n",
      "Epoch 40, train loss: 0.02187766134738922\n",
      "Epoch 50, train loss: 0.010519650764763355\n",
      "Epoch 60, train loss: 0.006160610821098089\n",
      "Epoch 70, train loss: 0.004049394745379686\n",
      "Epoch 80, train loss: 0.002863855566829443\n",
      "Epoch 90, train loss: 0.002130343345925212\n",
      "Epoch 100, train loss: 0.0016444830689579248\n",
      "Epoch 110, train loss: 0.001305481418967247\n",
      "Epoch 120, train loss: 0.0010592567268759012\n",
      "Epoch 130, train loss: 0.0008748586405999959\n",
      "Epoch 140, train loss: 0.0007331030792556703\n",
      "Epoch 150, train loss: 0.0006217090412974358\n",
      "Epoch 160, train loss: 0.0005326353712007403\n",
      "Epoch 170, train loss: 0.0004603703855536878\n",
      "Epoch 180, train loss: 0.00040102246566675603\n",
      "Epoch 190, train loss: 0.00035166851012036204\n",
      "Epoch 200, train loss: 0.00031020594178698957\n",
      "Epoch 210, train loss: 0.0002750381245277822\n",
      "Epoch 220, train loss: 0.0002449968596920371\n",
      "Epoch 230, train loss: 0.0002191594976466149\n",
      "Epoch 240, train loss: 0.00019679032266139984\n",
      "Epoch 250, train loss: 0.000177321198862046\n",
      "Epoch 260, train loss: 0.00016028083337005228\n",
      "Epoch 270, train loss: 0.0001452955330023542\n",
      "Epoch 280, train loss: 0.00013205190771259367\n",
      "Epoch 290, train loss: 0.0001203039864776656\n",
      "Epoch 300, train loss: 0.00010984224354615435\n",
      "Epoch 310, train loss: 0.00010048972035292536\n",
      "Epoch 320, train loss: 9.210915595758706e-05\n",
      "Epoch 330, train loss: 8.456366776954383e-05\n",
      "Epoch 340, train loss: 7.776492566335946e-05\n",
      "Epoch 350, train loss: 7.161770190577954e-05\n",
      "Epoch 360, train loss: 6.604821828659624e-05\n",
      "Epoch 370, train loss: 6.0990518250036985e-05\n",
      "Epoch 380, train loss: 5.6385062634944916e-05\n",
      "Epoch 390, train loss: 5.218999649514444e-05\n",
      "Epoch 400, train loss: 4.8356268962379545e-05\n",
      "Epoch 410, train loss: 4.485001409193501e-05\n",
      "Epoch 420, train loss: 4.16339207731653e-05\n",
      "Epoch 430, train loss: 3.867885607178323e-05\n",
      "Epoch 440, train loss: 3.596378519432619e-05\n",
      "Epoch 450, train loss: 3.3464420994278044e-05\n",
      "Epoch 460, train loss: 3.1160008802544326e-05\n",
      "Epoch 470, train loss: 2.9036023988737725e-05\n",
      "Epoch 480, train loss: 2.70721830020193e-05\n",
      "Epoch 490, train loss: 2.5257211746065877e-05\n",
      "Epoch 500, train loss: 2.358115671086125e-05\n",
      "Epoch 510, train loss: 2.2023166820872575e-05\n",
      "Epoch 520, train loss: 2.057978963421192e-05\n",
      "Epoch 530, train loss: 1.924098251038231e-05\n",
      "Epoch 540, train loss: 1.7999196643359028e-05\n",
      "Epoch 550, train loss: 1.6845591744640842e-05\n",
      "Epoch 560, train loss: 1.576638169353828e-05\n",
      "Epoch 570, train loss: 1.4766973436053377e-05\n",
      "Epoch 580, train loss: 1.3832283912051935e-05\n",
      "Epoch 590, train loss: 1.2963797416887246e-05\n",
      "Epoch 600, train loss: 1.2153979696449824e-05\n",
      "Epoch 610, train loss: 1.1399101822462399e-05\n",
      "Epoch 620, train loss: 1.0692645446397364e-05\n",
      "Epoch 630, train loss: 1.0032470527221449e-05\n",
      "Epoch 640, train loss: 9.41662165132584e-06\n",
      "Epoch 650, train loss: 8.842491297400557e-06\n",
      "Epoch 660, train loss: 8.304865332320333e-06\n",
      "Epoch 670, train loss: 7.800765160936862e-06\n",
      "Epoch 680, train loss: 7.33074693926028e-06\n",
      "Epoch 690, train loss: 6.890249551361194e-06\n",
      "Epoch 700, train loss: 6.476200269389665e-06\n",
      "Epoch 710, train loss: 6.088784175517503e-06\n",
      "Epoch 720, train loss: 5.7252082115155645e-06\n",
      "Epoch 730, train loss: 5.388360477809329e-06\n",
      "Epoch 740, train loss: 5.069578492111759e-06\n",
      "Epoch 750, train loss: 4.767279278894421e-06\n",
      "Epoch 760, train loss: 4.487518253881717e-06\n",
      "Epoch 770, train loss: 4.223122232360765e-06\n",
      "Epoch 780, train loss: 3.974558239860926e-06\n",
      "Epoch 790, train loss: 3.743688239410403e-06\n",
      "Epoch 800, train loss: 3.523342456901446e-06\n",
      "Epoch 810, train loss: 3.318735252832994e-06\n",
      "Epoch 820, train loss: 3.125955117866397e-06\n",
      "Epoch 830, train loss: 2.9437915145535953e-06\n",
      "Epoch 840, train loss: 2.7726175630959915e-06\n",
      "Epoch 850, train loss: 2.610756155263516e-06\n",
      "Epoch 860, train loss: 2.461746817061794e-06\n",
      "Epoch 870, train loss: 2.3205602701636963e-06\n",
      "Epoch 880, train loss: 2.1877553990634624e-06\n",
      "Epoch 890, train loss: 2.061842224065913e-06\n",
      "Epoch 900, train loss: 1.9443109522399027e-06\n",
      "Epoch 910, train loss: 1.8324601569474908e-06\n",
      "Epoch 920, train loss: 1.727780500004883e-06\n",
      "Epoch 930, train loss: 1.6305515373460366e-06\n",
      "Epoch 940, train loss: 1.5384446214739e-06\n",
      "Epoch 950, train loss: 1.4513667565552169e-06\n",
      "Epoch 960, train loss: 1.3674551837539184e-06\n",
      "Epoch 970, train loss: 1.2892247696072445e-06\n",
      "Epoch 980, train loss: 1.2173271670690156e-06\n",
      "Epoch 990, train loss: 1.1489686357890605e-06\n",
      "tensor([44, 38, 40, 18,  3,  7, 57, 22, 31, 24, 29, 19, 51, 43, 34, 59, 53, 36,\n",
      "        30, 34, 57, 56, 58, 37, 56, 56, 26, 54, 45, 41, 24, 60, 41, 45, 38, 51,\n",
      "        41, 30, 27, 53, 60, 46, 55, 52, 41, 47, 44, 29, 33, 59, 53, 24, 61, 44,\n",
      "        14, 30, 18, 62, 18, 22, 20, 18, 40, 37, 44, 25, 14, 33, 40, 22, 46, 18,\n",
      "        50, 11, 52, 58, 60, 17, 33, 18, 25, 33, 52, 25, 59, 48, 40, 31, 31, 46,\n",
      "        43, 31, 40, 15, 16, 48, 39, 29, 57, 37, 61, 35, 35, 53, 32, 24, 10, 43,\n",
      "        28, 51, 45, 20, 43, 46, 56, 41, 34, 34, 21, 61, 53, 36, 53, 38, 41, 56,\n",
      "        19, 56, 55, 60, 44, 28, 46, 34, 41, 51, 42, 53, 41, 43, 12, 41, 52, 10,\n",
      "        15, 25, 36, 59, 54, 53, 15, 39, 25, 45, 41, 54, 41, 52, 56, 51, 53, 42,\n",
      "        36, 31, 52, 43, 44, 43, 62, 27, 46, 53, 21, 26, 32, 53, 41, 52, 49, 23,\n",
      "        18, 45, 31, 14, 58, 30, 30, 26, 30, 38, 39, 59, 52, 43, 42, 43, 44, 35,\n",
      "         8, 55, 54, 26, 18, 45, 42, 16, 15, 59, 58,  9, 34, 36, 41, 39, 41, 51,\n",
      "        43, 14, 30, 51, 55, 27, 10, 49, 27, 57, 44, 23, 61, 62, 60, 44, 25, 42,\n",
      "        50, 62, 57, 37, 33, 48, 28, 15, 42, 53,  8, 35, 46, 21, 50, 22,  5,  7,\n",
      "        30, 18, 45, 43, 58, 33, 43, 50, 41, 60, 28, 48, 19, 37, 30, 50, 52, 25,\n",
      "        61, 20, 27, 22, 37, 45, 55, 21, 36, 60, 40, 46, 29, 34, 35, 17, 52, 63,\n",
      "        48, 42, 23, 47, 22, 45, 56, 29, 44, 55, 35, 38, 33, 42, 36, 29, 43, 63,\n",
      "        32, 38, 22, 49, 50, 43, 45, 18, 46, 44, 18, 33, 38, 40, 62, 63, 50, 57,\n",
      "        51, 52, 58, 41, 22, 56, 18, 12, 10, 53, 63, 28, 33, 45, 62, 51, 60, 50,\n",
      "        50, 54, 59, 60, 36, 47, 33, 43, 60, 51, 47, 39, 58, 62, 44, 23, 13,  8,\n",
      "        35, 44, 48, 38, 63, 53, 58, 49, 35, 10, 58, 58, 52, 32, 53, 59, 59, 62,\n",
      "        61, 50, 40, 58, 40, 35, 57, 56, 39, 48, 51, 38, 61, 49, 34, 54, 34, 40,\n",
      "        62, 52, 11, 60, 35, 48, 44, 48, 54, 43, 53, 13, 58, 62, 14, 36, 62, 34,\n",
      "        38, 55, 41, 27, 31, 57, 25, 51, 62, 51, 13, 35, 53, 53, 29, 53, 31, 49,\n",
      "        35, 39, 22,  7, 61, 60, 53, 14, 62, 52, 34, 37, 52, 56, 26, 41, 33, 54,\n",
      "        37, 44, 27, 32, 58, 62, 21, 31, 38, 49, 33, 48, 17,  1, 28, 54, 52, 24,\n",
      "        41, 61, 53, 59, 45, 15, 54, 44, 57, 63, 23, 58, 33, 59, 27, 27, 41, 58,\n",
      "        51, 47, 60, 48, 27, 62,  5, 55, 44, 41, 53, 25, 59, 58, 59, 22, 51, 25,\n",
      "        28, 57, 61, 38, 26, 61, 62, 30, 47, 52, 33, 61, 38, 57, 38, 32, 15, 38,\n",
      "        16, 63, 46, 55, 24, 44, 49, 26, 39, 49, 56,  7, 39, 62, 57, 61, 48, 61,\n",
      "        41, 54, 25, 36, 36, 56, 16, 37, 21, 48, 41, 12, 63, 43, 57, 55, 18,  8,\n",
      "        55, 26, 13,  6, 27, 22, 36, 52, 58, 43, 38, 16, 42, 31, 59, 31, 54, 57,\n",
      "        45, 47, 55, 61, 60, 45, 27, 39, 50, 37, 24, 40, 17, 44, 30, 35, 49, 11,\n",
      "        61,  1,  6, 60, 12, 61, 34, 55, 43, 20, 61, 61, 42, 50, 48, 20, 61, 54,\n",
      "        28, 34, 52, 52, 35, 56, 61, 26, 61, 21, 33, 60, 40, 63, 60, 51, 16, 58,\n",
      "        46, 55, 54, 47, 24, 42, 34, 10, 44, 41, 62, 48, 19, 63, 56, 31, 39, 59,\n",
      "        44, 18, 52, 61, 29, 31, 59, 14, 62, 62, 54, 48, 13, 63, 62, 48, 20, 62,\n",
      "        32, 36, 53, 24, 25, 40, 58, 63, 53, 27, 63, 10, 16, 16, 35, 29, 18, 58,\n",
      "         5, 60, 50, 17, 46, 49, 45, 40, 63, 61, 63, 38, 18, 55, 16, 56, 49, 49,\n",
      "        56,  5, 52, 52, 38, 45, 38, 52, 61, 38, 56, 61, 59, 28, 43, 39, 40, 53,\n",
      "        30, 49, 34, 36, 42, 29, 19, 38, 57, 21, 10, 37, 36, 12, 41, 58, 26, 60,\n",
      "        57, 30, 36, 58, 38, 31, 60, 51, 11, 60, 27, 11, 17, 56, 49, 50, 54, 47,\n",
      "        31, 53, 19, 61, 36, 59, 42, 51, 47, 59, 10, 35, 55, 23, 16, 43, 15, 45,\n",
      "         6, 58, 16, 58, 41, 45, 57, 55, 55, 60, 61, 46, 51, 38, 40, 41, 34, 12,\n",
      "        37, 53, 33, 43, 45, 19, 41, 26, 31, 32, 14, 61, 34, 51, 19, 63, 52, 39,\n",
      "        56, 56, 49, 48, 54, 52, 43, 29, 46, 57, 37, 42, 62, 47, 56, 56, 20, 31,\n",
      "        55, 24, 50, 49, 45, 43, 28, 55, 47, 55,  9, 49, 63, 48, 33, 53, 46, 44,\n",
      "        37, 36, 21, 59, 56, 59, 42, 40, 20, 59, 55, 38, 48, 50, 58, 62,  8, 40,\n",
      "        27, 26, 60, 57, 54, 43, 27, 23, 59, 63, 62, 42, 41, 21, 31, 24, 45, 57,\n",
      "        55, 44, 50, 50, 21, 60, 48, 37, 23, 32, 57, 40, 56, 35, 40, 55, 45, 15,\n",
      "        17, 43, 22, 35, 58, 43, 12, 43, 51, 56, 57, 51, 30, 51, 33, 52, 56, 51,\n",
      "        45, 19, 49, 60, 26, 39, 55, 47, 60, 45, 37, 35, 57, 45, 21, 60,  7, 40,\n",
      "        51, 28, 59, 12,  5,  8, 48, 53, 50, 61, 57, 40, 39, 39, 33, 19, 45, 13,\n",
      "        25, 10, 47, 41, 36, 24, 12,  3, 44, 59, 36, 21, 26, 63, 25, 37, 56, 37,\n",
      "        38, 20, 55, 47, 62, 23, 18, 56, 50, 60, 50, 29, 46, 45, 56, 38, 52, 49,\n",
      "        21, 55, 57, 19, 54, 56, 15, 54, 51, 18, 24, 59, 45, 24, 31, 32, 43, 44,\n",
      "        47, 50, 33, 42, 48, 18, 39, 49, 17, 18, 40, 19, 51, 38, 57, 45, 57, 47,\n",
      "        42, 41, 38, 53, 41, 59, 51, 53, 22, 28, 43, 32, 55, 46, 16, 59, 62, 58,\n",
      "        59, 55, 58, 45,  7, 49, 35, 24, 25, 45, 17, 44, 59, 63, 30, 44, 41, 51,\n",
      "        12, 37, 17, 16, 59, 17, 14, 54, 57, 36, 58, 45, 37, 41, 41, 34, 30, 28,\n",
      "        44, 50, 48, 46, 30, 48, 63, 48, 31, 58, 55, 60, 31, 47, 46, 63, 50, 42,\n",
      "        38, 26, 55, 54, 36, 52, 41, 41, 29, 11, 59, 54, 38, 56, 30, 32, 40, 51,\n",
      "        13, 45, 24, 49, 56, 56, 60, 28, 10, 60, 30, 18, 44, 17, 42, 44, 40, 48,\n",
      "        17, 10, 54, 55, 42, 54, 44, 48, 31, 30, 24, 53, 34, 54, 43, 50, 49, 53,\n",
      "        45, 20, 63, 10, 57, 30, 26, 37, 34, 20, 48, 34,  5, 58,  6,  7, 41, 12,\n",
      "        59, 61, 54, 23,  9, 45, 30, 45, 51, 63, 49, 26, 44, 39, 28, 39, 42, 54,\n",
      "        44,  9, 20, 52, 26, 49, 36, 17, 52, 42, 53, 62, 48, 21, 40, 55, 45, 20,\n",
      "        44, 55, 16,  7, 46, 44, 16, 54, 57, 43, 55, 31, 60, 33, 52, 20, 24, 50,\n",
      "        33, 43, 48, 37, 62]) tensor([44, 51, 40, 18,  8, 41, 57, 22, 31, 24, 29, 19, 51, 43, 34, 59, 57, 32,\n",
      "        30, 34, 57, 62, 58, 37, 56, 56, 54, 22, 45, 41, 24, 60, 41, 45, 38, 51,\n",
      "        61, 30, 21, 53, 60, 53, 55, 52, 43, 47, 44, 29, 33, 59, 53, 24, 61, 44,\n",
      "        14, 30, 20, 62, 18, 22, 20, 18, 40, 37, 44, 25, 14, 33, 40, 22, 46, 18,\n",
      "        56, 11, 52, 58, 60, 17, 33, 19, 25, 33, 52, 25, 59, 48, 40, 31, 31, 46,\n",
      "        43, 31, 40, 15, 11, 48, 46, 29, 57, 37, 61, 35, 35, 53, 32, 37, 12, 48,\n",
      "        28, 51, 45, 21, 43, 46, 56, 41, 34, 34, 21, 61, 63, 36, 53, 46, 41, 56,\n",
      "        21, 56, 55, 60, 44, 28, 46, 34, 41, 51, 42, 53, 49, 43, 13, 53, 52, 19,\n",
      "        15, 25, 46, 59, 54, 53, 15, 39, 25, 45, 41, 54, 41, 52, 56, 51, 53, 53,\n",
      "        36, 47, 54, 43, 34, 43, 62, 27, 46, 53, 21, 26, 32, 53, 41, 52, 49, 48,\n",
      "        18, 45, 31, 23, 58, 51, 43, 59, 30, 38, 39, 54, 52, 43, 42, 43, 44, 35,\n",
      "         8, 55, 54, 26, 27, 45, 42, 16, 15, 59, 58,  9, 34, 36, 41, 39, 41, 59,\n",
      "        43, 14, 42, 51, 62, 27, 20, 49, 28, 57, 44, 23, 61, 62, 60, 44, 25, 42,\n",
      "        37, 62, 57, 37, 33, 48, 28, 15, 42, 53,  8, 38, 46, 21, 50, 22,  7,  4,\n",
      "        32, 18, 45, 43, 58, 33, 43, 50, 51, 60, 28, 48, 19, 37, 30, 59, 57, 25,\n",
      "        61, 27, 27, 22, 42, 45, 57, 21, 32, 60, 40, 46, 29, 34, 35, 22, 52, 63,\n",
      "        48, 34, 23, 47, 22, 45, 56, 32, 44, 55, 35, 38, 33, 42, 36, 38, 43, 63,\n",
      "        32, 38, 22, 49, 50, 43, 49, 18, 46, 44, 18, 40, 38, 40, 62, 63, 50, 57,\n",
      "        51, 52, 58, 41, 32, 56, 28, 12, 10, 53, 63, 32, 33, 45, 62, 51, 60, 50,\n",
      "        50, 54, 59, 60, 36, 47, 33, 43, 60, 51, 47, 39, 58, 45, 55, 23, 23, 14,\n",
      "        35, 44, 48, 38, 63, 53, 58, 49, 35, 10, 62, 58, 52, 32, 53, 59, 59, 62,\n",
      "        61, 50, 40, 57, 38, 35, 57, 56, 39, 49, 38, 38, 61, 49, 34, 54, 49, 40,\n",
      "        62, 52, 14, 60, 35, 48, 44, 48, 57, 44, 53, 13, 58, 62, 14, 36, 62, 46,\n",
      "        38, 55, 41, 27, 31, 57, 25, 51, 62, 51, 14, 35, 53, 53, 35, 53, 32, 49,\n",
      "        35, 39, 12, 14, 61, 60, 53, 14, 62, 54, 39, 37, 52, 63, 49, 41, 33, 54,\n",
      "        37, 44, 27, 32, 58, 62, 21, 36, 38, 49, 33, 48, 17,  1, 28, 54, 52, 24,\n",
      "        41, 61, 53, 59, 45, 15, 54, 44, 57, 56, 23, 58, 33, 62, 27, 27, 45, 58,\n",
      "        48, 49, 60, 48, 42, 62,  9, 55, 44, 41, 53, 25, 59, 58, 59, 23, 51, 27,\n",
      "        35, 57, 61, 38, 26, 61, 62, 63, 47, 54, 56, 61, 38, 57, 49, 32, 15, 56,\n",
      "        16, 63, 46, 55, 24, 44, 49, 26, 39, 49, 56,  7, 39, 62, 57, 61, 48, 61,\n",
      "        47, 54, 25, 36, 36, 56, 16, 37, 21, 48, 41, 12, 63, 43, 57, 55, 18,  9,\n",
      "        55, 26, 13,  6, 27, 22, 36, 52, 58, 49, 38, 16, 42, 31, 59, 31, 54, 61,\n",
      "        45, 47, 55, 61, 60, 45, 13, 39, 50, 46, 24, 40, 17, 44, 30, 35, 49, 11,\n",
      "        61,  4,  8, 60, 12, 61, 34, 55, 43, 20, 61, 61, 42, 50, 48, 20, 61, 54,\n",
      "        28, 34, 52, 52, 35, 56, 61, 26, 61, 21, 33, 60, 40, 63, 60, 43, 16, 58,\n",
      "        46, 60, 54, 47, 24, 42, 34, 10, 44, 41, 62, 48, 19, 63, 56, 31, 39, 59,\n",
      "        44, 18, 52, 61, 29, 59, 59, 14, 62, 62, 54, 48, 29, 63, 62, 48, 22, 62,\n",
      "        32, 36, 53, 30, 25, 40, 58, 63, 53, 27, 63, 10, 15, 56, 35, 29, 18, 58,\n",
      "         9, 60, 50, 27, 46, 50, 45, 40, 63, 61, 63, 38, 23, 55, 16, 56, 49, 49,\n",
      "        56, 14, 52, 52, 38, 45, 38, 52, 61, 38, 56, 61, 59, 28, 43, 39, 40, 53,\n",
      "        47, 49, 34, 36, 42, 29, 19, 38, 57, 21, 10, 37, 36, 12, 54, 58, 26, 60,\n",
      "        57, 29, 36, 58, 38, 54, 60, 51, 11, 60, 27, 11, 17, 56, 49, 50, 54, 47,\n",
      "        31, 53, 19, 61, 36, 59, 42, 51, 47, 59,  6, 35, 55, 23, 16, 19, 15, 46,\n",
      "         4, 58, 16, 58, 56, 45, 57, 55, 55, 60, 61, 46, 51, 38, 40, 41, 34, 12,\n",
      "        38, 53, 37, 55, 45, 19, 41, 26, 31, 32, 14, 61, 34, 52, 19, 63, 52, 39,\n",
      "        56, 56, 42, 62, 54, 52, 43, 44, 46, 57, 52, 42, 62, 48, 56, 59, 20, 31,\n",
      "        55, 24, 52, 49, 45, 51, 28, 55, 47, 55,  9, 49, 63, 48, 33, 58, 48, 44,\n",
      "        37, 36, 21, 63, 56, 59, 42, 40, 20, 59, 55, 38, 48, 50, 58, 62,  4, 40,\n",
      "        27, 26, 60, 49, 54, 43, 27, 23, 59, 63, 62, 42, 41, 26, 31, 25, 45, 57,\n",
      "        55, 44, 50, 57, 21, 60, 48, 37, 25, 32, 57, 40, 56, 35, 40, 55, 63, 51,\n",
      "        17, 43, 22, 35, 58, 43, 21, 47, 51, 56, 62, 51, 30, 36, 33, 52, 56, 62,\n",
      "        50, 19, 49, 60, 26, 39, 55, 47, 60, 45, 48, 35, 57, 45, 21, 60,  4, 40,\n",
      "        51, 28, 28,  6,  8,  8, 48, 53, 50, 61, 57, 40, 40, 39, 33, 19, 45, 13,\n",
      "        25, 21, 47, 41, 40, 24, 12,  3, 44, 59, 39, 21, 30, 63, 25, 37, 56, 37,\n",
      "        38, 20, 55, 47, 62, 23, 18, 56, 55, 60, 50, 29, 46, 47, 56, 40, 52, 55,\n",
      "        21, 55, 57, 19, 54, 56, 23, 54, 51, 19, 28, 59, 45, 51, 32, 32, 43, 44,\n",
      "        47, 50, 33, 42, 48, 18, 39, 49, 17, 18, 40, 19, 56, 38, 57, 45, 57, 47,\n",
      "        42, 41, 38, 53, 41, 59, 51, 53, 25, 28, 43, 32, 55, 46, 16, 59, 62, 58,\n",
      "        59, 55, 63, 45,  7, 49, 35, 54, 40, 60, 19, 44, 59, 63, 30, 47, 57, 51,\n",
      "        12, 37, 20, 16, 59, 25, 14, 54, 57, 36, 58, 45, 37, 41, 41, 34, 30, 28,\n",
      "        44, 50, 48, 46, 30, 48, 63, 48, 31, 58, 55, 60, 31, 55, 46, 63, 50, 42,\n",
      "        38, 26, 55, 54, 36, 52, 41, 41, 29, 11, 59, 54, 42, 59, 30, 32, 41, 51,\n",
      "        13, 45, 47, 49, 56, 56, 60, 28, 10, 60, 30, 18, 44, 17, 61, 44, 40, 48,\n",
      "        17, 12, 54, 55, 42, 54, 44, 48, 31, 30, 24, 53, 34, 54, 43, 50, 49, 53,\n",
      "        45, 23, 63, 10, 57, 30, 26, 37, 34, 20, 48, 49,  6, 58,  6, 10, 41, 12,\n",
      "        59, 39, 63, 23,  9, 45, 30, 45, 51, 63, 49, 26, 44, 39, 28, 39, 43, 54,\n",
      "        44,  4, 20, 58, 26, 49, 36, 17, 52, 46, 54, 62, 48, 21, 40, 55, 45, 30,\n",
      "        48, 55, 16,  7, 49, 44, 16, 54, 57, 43, 55, 31, 60, 33, 52, 20, 25, 50,\n",
      "        33, 43, 48, 37, 62])\n",
      "Test accuracy: 0.8242473602294922\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=10000)\n",
    "losses = train_model(model, 1000, 128, 10, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
